{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66b2f847",
   "metadata": {},
   "source": [
    "# üß† TAB 5: Modeling Strategy & Baseline Training (Industry Reality)\n",
    "\n",
    "**Project:** E-commerce Customer Churn Prediction  \n",
    "**Dataset:** 5,630 customers | Churn Rate: ~16.84%  \n",
    "**Philosophy:** Models don't win churn projects. **Decisions do.**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "> Prove that your model is **better than doing nothing** and **safe to deploy**.\n",
    "\n",
    "**NOT** chasing accuracy. We're building **decision systems**, not models.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã TAB 5 Checklist\n",
    "\n",
    "### Phase 1: Dumb Baseline (MANDATORY)\n",
    "- ‚úÖ Majority-class baseline\n",
    "- ‚úÖ Expected accuracy ‚âà 83.16%\n",
    "- ‚úÖ Recall (churn) = 0\n",
    "- ‚úÖ Business value = 0\n",
    "\n",
    "### Phase 2: First Real Model - Logistic Regression\n",
    "- ‚úÖ Interpretable & probability-calibrated\n",
    "- ‚úÖ Use class weights (NOT SMOTE)\n",
    "- ‚úÖ No hyperparameter tuning initially\n",
    "- ‚úÖ Recall (churn) > 0.5 target\n",
    "\n",
    "### Phase 3: Threshold Tuning (CRITICAL)\n",
    "- ‚úÖ Default threshold ‚â† 0.5\n",
    "- ‚úÖ Business-aligned threshold selection\n",
    "- ‚úÖ Target top 10-20% highest-risk customers\n",
    "\n",
    "### Phase 4: Tree Ensemble (ONLY AFTER BASELINE)\n",
    "- ‚úÖ XGBoost / LightGBM / CatBoost\n",
    "- ‚úÖ Same features, same split\n",
    "- ‚úÖ Compare interpretability vs performance\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è CRITICAL RULES:**\n",
    "1. If your ML model doesn't beat majority-class baseline **meaningfully**, it's useless\n",
    "2. False negatives are **more expensive** than false positives in churn\n",
    "3. Probability ‚â† Decision (threshold tuning is business decision)\n",
    "4. If 2 very different models fail ‚Üí data/feature problem\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31328c55",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Import Libraries & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"Scikit-learn ready | XGBoost ready | LightGBM ready | CatBoost ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee25a0e",
   "metadata": {},
   "source": [
    "## üì• Step 2: Load Feature-Engineered Data\n",
    "\n",
    "**Source:** Output from TAB 4 (Feature Engineering)  \n",
    "**Expected:** Clean, no leakage, train-test split done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature-engineered data\n",
    "# NOTE: This assumes you've run the feature engineering notebook (TAB 4)\n",
    "# and saved the processed data\n",
    "\n",
    "try:\n",
    "    # Try to load saved processed data (Phase 2 = Baseline + Controlled features)\n",
    "    X_train = pd.read_csv('../data/processed/X_train_phase2.csv')\n",
    "    X_test = pd.read_csv('../data/processed/X_test_phase2.csv')\n",
    "    y_train = pd.read_csv('../data/processed/y_train.csv').squeeze()\n",
    "    y_test = pd.read_csv('../data/processed/y_test.csv').squeeze()\n",
    "    \n",
    "    print(\"‚úÖ Loaded pre-processed data from TAB 4 (Phase 2: Baseline + Controlled)\")\n",
    "    print(f\"\\nTrain shape: {X_train.shape}\")\n",
    "    print(f\"Test shape: {X_test.shape}\")\n",
    "    print(f\"Features: {X_train.shape[1]} (18 original + 6 missing flags + 2 engineered)\")\n",
    "    print(f\"\\nTrain churn rate: {y_train.mean()*100:.2f}%\")\n",
    "    print(f\"Test churn rate: {y_test.mean()*100:.2f}%\")\n",
    "    \n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Processed data not found!\")\n",
    "    print(\"\\nFalling back to raw data + basic preprocessing...\")\n",
    "    \n",
    "    # Load raw data\n",
    "    df = pd.read_csv('../data/raw/ecommerce_churn.csv')\n",
    "    \n",
    "    # Drop CustomerID\n",
    "    df = df.drop('CustomerID', axis=1)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop('Churn', axis=1)\n",
    "    y = df['Churn']\n",
    "    \n",
    "    # Basic preprocessing: Fill missing values\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Median for numerical\n",
    "    for col in numerical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "    \n",
    "    # Mode for categorical\n",
    "    for col in categorical_cols:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # Label encode categorical\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Basic preprocessing completed\")\n",
    "    print(f\"\\nTrain shape: {X_train.shape}\")\n",
    "    print(f\"Test shape: {X_test.shape}\")\n",
    "    print(f\"\\nTrain churn rate: {y_train.mean()*100:.2f}%\")\n",
    "    print(f\"Test churn rate: {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7a106",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ DUMB BASELINE (MANDATORY)\n",
    "\n",
    "### ‚ö†Ô∏è If you skip this, you fail interviews.\n",
    "\n",
    "**Strategy:** Predict \"No Churn\" for everyone  \n",
    "**Expected Accuracy:** ~83.16%  \n",
    "**Recall (churn):** 0  \n",
    "**Business Value:** 0\n",
    "\n",
    "üëâ This is your **floor**, not your competitor.\n",
    "\n",
    "**Rule:** If your ML model doesn't beat this **meaningfully**, it's useless.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc19357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumb Baseline: Predict majority class (No Churn = 0)\n",
    "y_pred_dumb = np.zeros(len(y_test))\n",
    "\n",
    "# Calculate metrics\n",
    "dumb_accuracy = accuracy_score(y_test, y_pred_dumb)\n",
    "dumb_precision = precision_score(y_test, y_pred_dumb, zero_division=0)\n",
    "dumb_recall = recall_score(y_test, y_pred_dumb, zero_division=0)\n",
    "dumb_f1 = f1_score(y_test, y_pred_dumb, zero_division=0)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ü§ñ DUMB BASELINE: Predict 'No Churn' for Everyone\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Accuracy:  {dumb_accuracy:.4f} ({dumb_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {dumb_precision:.4f}\")\n",
    "print(f\"Recall:    {dumb_recall:.4f}\")\n",
    "print(f\"F1-Score:  {dumb_f1:.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_dumb = confusion_matrix(y_test, y_pred_dumb)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_dumb)\n",
    "print(\"\\n[Interpretation]\")\n",
    "print(f\"True Negatives:  {cm_dumb[0,0]:,} ‚úÖ (Correctly predicted No Churn)\")\n",
    "print(f\"False Positives: {cm_dumb[0,1]:,} ‚ùå (Predicted Churn, but didn't)\")\n",
    "print(f\"False Negatives: {cm_dumb[1,0]:,} ‚ùå (Predicted No Churn, but CHURNED)\")\n",
    "print(f\"True Positives:  {cm_dumb[1,1]:,} ‚úÖ (Correctly predicted Churn)\")\n",
    "\n",
    "# Business Translation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä BUSINESS TRANSLATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Saved churners:         {cm_dumb[1,1]:,} (0%)\")\n",
    "print(f\"Lost customers:         {cm_dumb[1,0]:,} ({cm_dumb[1,0]/len(y_test)*100:.1f}%)\")\n",
    "print(f\"Wasted retention:       {cm_dumb[0,1]:,}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚ö†Ô∏è This baseline catches ZERO churners. Useless for business.\")\n",
    "print(\"\\n‚úÖ ANY ML model MUST beat this to be valuable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Dumb Baseline\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm_dumb, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            cbar_kws={'label': 'Count'}, ax=ax)\n",
    "\n",
    "ax.set_title('Dumb Baseline: Confusion Matrix\\n(Predict No Churn for Everyone)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìå Key Insight: All actual churners (bottom row) are missed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01688c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ FIRST REAL MODEL - LOGISTIC REGRESSION (NON-NEGOTIABLE)\n",
    "\n",
    "### Why Logistic Regression FIRST?\n",
    "\n",
    "‚úÖ **Interpretable** - Can explain to business  \n",
    "‚úÖ **Stable** - Consistent results  \n",
    "‚úÖ **Probability-calibrated** - Outputs are actual probabilities  \n",
    "‚úÖ **Forces feature discipline** - Can't hide behind complexity  \n",
    "‚úÖ **Easy to explain** - Business stakeholders understand it\n",
    "\n",
    "üö© **If someone starts with XGBoost ‚Üí RED FLAG**\n",
    "\n",
    "---\n",
    "\n",
    "### Setup Principles (NO CODE YET)\n",
    "\n",
    "1. Use **ONLY baseline features** (from TAB 4)\n",
    "2. Use **class weights**, not SMOTE\n",
    "3. No hyperparameter tuning initially\n",
    "4. Default threshold ‚â† 0.5 (we'll fix this later)\n",
    "\n",
    "---\n",
    "\n",
    "### What Success Looks Like (Baseline)\n",
    "\n",
    "Don't fixate on numbers, fixate on **direction**:\n",
    "\n",
    "- ‚úÖ Recall (churn) > **0.5**\n",
    "- ‚úÖ Precision not collapsing (<0.2 is bad)\n",
    "- ‚úÖ ROC-AUC > **0.70**\n",
    "- ‚úÖ Confusion matrix shows **actual churners caught**\n",
    "\n",
    "**If this fails ‚Üí go back to features, NOT models.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (Logistic Regression needs scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features scaled using StandardScaler\")\n",
    "print(f\"Train shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a16c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression with class weights\n",
    "# class_weight='balanced' automatically handles imbalance\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    class_weight='balanced',  # Handle imbalance\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\"‚úÖ Model trained successfully!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_roc_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ LOGISTIC REGRESSION RESULTS (Threshold = 0.5)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Accuracy:  {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {lr_precision:.4f}\")\n",
    "print(f\"Recall:    {lr_recall:.4f}\")\n",
    "print(f\"F1-Score:  {lr_f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {lr_roc_auc:.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_lr)\n",
    "print(\"\\n[Business Translation]\")\n",
    "print(f\"True Negatives:  {cm_lr[0,0]:,} ‚úÖ (Correctly ignored)\")\n",
    "print(f\"False Positives: {cm_lr[0,1]:,} ‚ùå (Wasted retention effort)\")\n",
    "print(f\"False Negatives: {cm_lr[1,0]:,} ‚ùå (Lost customers)\")\n",
    "print(f\"True Positives:  {cm_lr[1,1]:,} ‚úÖ (Saved churners)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä BUSINESS IMPACT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Churners caught:        {cm_lr[1,1]:,} / {cm_lr[1,0] + cm_lr[1,1]:,} ({lr_recall*100:.1f}%)\")\n",
    "print(f\"Customers contacted:    {cm_lr[0,1] + cm_lr[1,1]:,} ({(cm_lr[0,1] + cm_lr[1,1])/len(y_test)*100:.1f}%)\")\n",
    "print(f\"Precision (efficiency): {lr_precision*100:.1f}%\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ccbf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Logistic Regression Results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'],\n",
    "            cbar_kws={'label': 'Count'}, ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression: Confusion Matrix\\n(Threshold = 0.5)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "\n",
    "# ROC Curve\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_pred_proba_lr)\n",
    "axes[1].plot(fpr_lr, tpr_lr, linewidth=2, label=f'Logistic Regression (AUC = {lr_roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[1].set_ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "axes[1].set_title('ROC Curve - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb2aeb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ THRESHOLD TUNING (THIS IS WHERE YOU LOOK SENIOR)\n",
    "\n",
    "### üéØ Probability ‚â† Decision\n",
    "\n",
    "Your model outputs **probability**, not truth.\n",
    "\n",
    "**Example:**\n",
    "- Customer A ‚Üí churn prob = 0.72\n",
    "- Customer B ‚Üí churn prob = 0.41\n",
    "\n",
    "**Business question:**\n",
    "> At what probability do we act?\n",
    "\n",
    "That's a **threshold decision**, not an ML one.\n",
    "\n",
    "---\n",
    "\n",
    "### Default Threshold = 0.5 ‚Üí Arbitrary\n",
    "\n",
    "### Industry-Style Thresholds:\n",
    "\n",
    "1. Target **top 10-20% highest-risk customers**\n",
    "2. Or maximize **Recall @ fixed Precision**\n",
    "3. Or minimize **expected cost**\n",
    "\n",
    "**You must be able to say:**\n",
    "\n",
    "> \"We tuned the threshold to catch ~70% churners while contacting ~25% customers.\"\n",
    "\n",
    "**That sentence alone upgrades you.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fece8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, y_pred_proba_lr)\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[0].plot(recall_vals, precision_vals, linewidth=2, color='purple')\n",
    "axes[0].set_xlabel('Recall (Churners Caught)', fontsize=12)\n",
    "axes[0].set_ylabel('Precision (Efficiency)', fontsize=12)\n",
    "axes[0].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=y_test.mean(), color='red', linestyle='--', \n",
    "                label=f'Baseline (No Skill) = {y_test.mean():.3f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Threshold Analysis\n",
    "f1_scores = 2 * (precision_vals * recall_vals) / (precision_vals + recall_vals + 1e-10)\n",
    "axes[1].plot(pr_thresholds, precision_vals[:-1], label='Precision', linewidth=2)\n",
    "axes[1].plot(pr_thresholds, recall_vals[:-1], label='Recall', linewidth=2)\n",
    "axes[1].plot(pr_thresholds, f1_scores[:-1], label='F1-Score', linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Threshold', fontsize=12)\n",
    "axes[1].set_ylabel('Score', fontsize=12)\n",
    "axes[1].set_title('Metrics vs Threshold', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=0.5, color='black', linestyle=':', alpha=0.5, label='Default (0.5)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold (maximize F1)\n",
    "optimal_idx = np.argmax(f1_scores[:-1])\n",
    "optimal_threshold = pr_thresholds[optimal_idx]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéØ OPTIMAL THRESHOLD ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Optimal Threshold (Max F1): {optimal_threshold:.3f}\")\n",
    "print(f\"Precision at optimal:       {precision_vals[optimal_idx]:.3f}\")\n",
    "print(f\"Recall at optimal:          {recall_vals[optimal_idx]:.3f}\")\n",
    "print(f\"F1-Score at optimal:        {f1_scores[optimal_idx]:.3f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcebd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business-Aligned Threshold Selection\n",
    "# Strategy: Contact top 20% highest-risk customers\n",
    "\n",
    "# Sort probabilities\n",
    "sorted_proba = np.sort(y_pred_proba_lr)[::-1]\n",
    "top_20_pct_idx = int(len(sorted_proba) * 0.20)\n",
    "business_threshold = sorted_proba[top_20_pct_idx]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä BUSINESS-ALIGNED THRESHOLD\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Strategy: Contact top 20% highest-risk customers\")\n",
    "print(f\"Business Threshold: {business_threshold:.3f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Apply business threshold\n",
    "y_pred_business = (y_pred_proba_lr >= business_threshold).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "business_accuracy = accuracy_score(y_test, y_pred_business)\n",
    "business_precision = precision_score(y_test, y_pred_business)\n",
    "business_recall = recall_score(y_test, y_pred_business)\n",
    "business_f1 = f1_score(y_test, y_pred_business)\n",
    "\n",
    "print(f\"\\nAccuracy:  {business_accuracy:.4f}\")\n",
    "print(f\"Precision: {business_precision:.4f}\")\n",
    "print(f\"Recall:    {business_recall:.4f}\")\n",
    "print(f\"F1-Score:  {business_f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_business = confusion_matrix(y_test, y_pred_business)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm_business)\n",
    "print(\"\\n[Business Translation]\")\n",
    "print(f\"Saved churners:         {cm_business[1,1]:,} / {cm_business[1,0] + cm_business[1,1]:,} ({business_recall*100:.1f}%)\")\n",
    "print(f\"Customers contacted:    {cm_business[0,1] + cm_business[1,1]:,} ({(cm_business[0,1] + cm_business[1,1])/len(y_test)*100:.1f}%)\")\n",
    "print(f\"Precision (efficiency): {business_precision*100:.1f}%\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ INTERVIEW-READY STATEMENT:\")\n",
    "print(f'   \"We tuned the threshold to {business_threshold:.3f} to catch {business_recall*100:.1f}% churners')\n",
    "print(f'    while contacting only {(cm_business[0,1] + cm_business[1,1])/len(y_test)*100:.1f}% of customers.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dcef12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ TREE ENSEMBLE (ONLY AFTER BASELINE)\n",
    "\n",
    "### You've earned the right to use:\n",
    "- XGBoost\n",
    "- LightGBM  \n",
    "- CatBoost\n",
    "\n",
    "### Rules:\n",
    "\n",
    "1. ‚úÖ Same features initially (no cheating)\n",
    "2. ‚úÖ Same train/test split\n",
    "3. ‚úÖ Same evaluation metrics\n",
    "4. ‚úÖ No blind SMOTE\n",
    "\n",
    "**If tree model improves AUC by 0.02-0.03, that's already good.**\n",
    "\n",
    "**If it improves nothing ‚Üí keep Logistic Regression.**\n",
    "\n",
    "Yes, that happens often.\n",
    "\n",
    "---\n",
    "\n",
    "### Why \"Better Metrics\" Can Still Be Worse\n",
    "\n",
    "Tree models often:\n",
    "- ‚úÖ Increase recall\n",
    "- ‚ùå Destroy probability calibration\n",
    "- ‚ùå Become harder to explain\n",
    "\n",
    "**So you must ask:**\n",
    "\n",
    "> \"Is the performance gain worth the loss in interpretability?\"\n",
    "\n",
    "That's an **engineering trade-off**, not an ML one.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'XGBoost': XGBClassifier(\n",
    "        scale_pos_weight=(len(y_train) - y_train.sum()) / y_train.sum(),\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        verbosity=0\n",
    "    ),\n",
    "    'LightGBM': LGBMClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        verbose=-1\n",
    "    ),\n",
    "    'CatBoost': CatBoostClassifier(\n",
    "        auto_class_weights='Balanced',\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    ),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üå≤ TRAINING TREE ENSEMBLE MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'Dumb Baseline': {\n",
    "        'Accuracy': dumb_accuracy,\n",
    "        'Precision': dumb_precision,\n",
    "        'Recall': dumb_recall,\n",
    "        'F1-Score': dumb_f1,\n",
    "        'ROC-AUC': 0.5\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': lr_accuracy,\n",
    "        'Precision': lr_precision,\n",
    "        'Recall': lr_recall,\n",
    "        'F1-Score': lr_f1,\n",
    "        'ROC-AUC': lr_roc_auc\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc,\n",
    "        'Model': model,\n",
    "        'Predictions': y_pred,\n",
    "        'Probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} trained\")\n",
    "    print(f\"   ROC-AUC: {roc_auc:.4f} | Recall: {recall:.4f} | Precision: {precision:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ All models trained successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a17e65",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ MODEL COMPARISON - HOW TO PRESENT IT\n",
    "\n",
    "### You compare models on:\n",
    "\n",
    "1. ‚úÖ **ROC-AUC** - Overall discriminative ability\n",
    "2. ‚úÖ **Recall** - How many churners we catch\n",
    "3. ‚úÖ **Precision** - How efficient we are\n",
    "4. ‚úÖ **Stability** - CV variance (not shown here, but important)\n",
    "5. ‚úÖ **Explainability** - Can we explain to business?\n",
    "\n",
    "**NOT on accuracy.**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå NEVER SAY:\n",
    "\n",
    "> \"XGBoost achieved 92% accuracy\"\n",
    "\n",
    "### ‚úÖ ALWAYS SAY:\n",
    "\n",
    "> \"XGBoost improved recall by 8% over Logistic Regression at similar precision.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']]\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string())\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Highlight best model per metric\n",
    "print(\"\\nüèÜ BEST MODELS PER METRIC:\")\n",
    "print(f\"Best ROC-AUC:   {results_df['ROC-AUC'].idxmax()} ({results_df['ROC-AUC'].max():.4f})\")\n",
    "print(f\"Best Recall:    {results_df['Recall'].idxmax()} ({results_df['Recall'].max():.4f})\")\n",
    "print(f\"Best Precision: {results_df['Precision'].idxmax()} ({results_df['Precision'].max():.4f})\")\n",
    "print(f\"Best F1-Score:  {results_df['F1-Score'].idxmax()} ({results_df['F1-Score'].max():.4f})\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c345c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['ROC-AUC', 'Recall', 'Precision', 'F1-Score']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Sort by metric\n",
    "    sorted_results = results_df.sort_values(metric, ascending=True)\n",
    "    \n",
    "    # Plot\n",
    "    bars = ax.barh(sorted_results.index, sorted_results[metric], color=colors[:len(sorted_results)])\n",
    "    \n",
    "    # Highlight best\n",
    "    best_idx = sorted_results[metric].idxmax()\n",
    "    best_bar_idx = list(sorted_results.index).index(best_idx)\n",
    "    bars[best_bar_idx].set_color('gold')\n",
    "    bars[best_bar_idx].set_edgecolor('black')\n",
    "    bars[best_bar_idx].set_linewidth(2)\n",
    "    \n",
    "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(sorted_results[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curves for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot each model\n",
    "for name in results.keys():\n",
    "    if name == 'Dumb Baseline':\n",
    "        continue\n",
    "    \n",
    "    if name == 'Logistic Regression':\n",
    "        y_proba = y_pred_proba_lr\n",
    "    else:\n",
    "        y_proba = results[name]['Probabilities']\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    auc = results[name]['ROC-AUC']\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "# Plot random classifier\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier (AUC = 0.500)')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curves - All Models', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìå Key Insight: Compare how models separate churners from non-churners\")\n",
    "print(\"   Higher AUC = Better discrimination ability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543a0fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ FAILURE MODES YOU MUST WATCH FOR\n",
    "\n",
    "### ‚ùå Logistic Regression works, trees don't\n",
    "‚Üí Features are linear ‚Üí **that's fine**\n",
    "\n",
    "### ‚ùå Trees work, LR fails\n",
    "‚Üí Non-linear interactions ‚Üí **explain carefully**\n",
    "\n",
    "### ‚ùå Both fail\n",
    "‚Üí **Feature problem, not model problem**\n",
    "\n",
    "---\n",
    "\n",
    "### Rule:\n",
    "**If 2 very different models fail ‚Üí data/feature issue.**\n",
    "\n",
    "Go back to TAB 4 (Feature Engineering), not TAB 5 (Modeling).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21722b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure Mode Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç FAILURE MODE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if models are performing well\n",
    "lr_auc = results['Logistic Regression']['ROC-AUC']\n",
    "best_tree_auc = max([results[m]['ROC-AUC'] for m in models.keys()])\n",
    "best_tree_name = [m for m in models.keys() if results[m]['ROC-AUC'] == best_tree_auc][0]\n",
    "\n",
    "print(f\"\\nLogistic Regression AUC: {lr_auc:.4f}\")\n",
    "print(f\"Best Tree Model AUC:     {best_tree_auc:.4f} ({best_tree_name})\")\n",
    "print(f\"Improvement:             {(best_tree_auc - lr_auc):.4f} ({(best_tree_auc - lr_auc)*100:.2f}%)\")\n",
    "\n",
    "# Diagnosis\n",
    "if lr_auc < 0.65 and best_tree_auc < 0.65:\n",
    "    print(\"\\n‚ùå FAILURE MODE: Both LR and Trees failing\")\n",
    "    print(\"   ‚Üí DIAGNOSIS: Feature problem, not model problem\")\n",
    "    print(\"   ‚Üí ACTION: Go back to TAB 4 (Feature Engineering)\")\n",
    "    \n",
    "elif lr_auc >= 0.70 and best_tree_auc < lr_auc:\n",
    "    print(\"\\n‚úÖ SUCCESS MODE: LR works, Trees don't improve\")\n",
    "    print(\"   ‚Üí DIAGNOSIS: Features are mostly linear\")\n",
    "    print(\"   ‚Üí ACTION: Keep Logistic Regression (interpretability wins)\")\n",
    "    \n",
    "elif best_tree_auc > lr_auc + 0.02:\n",
    "    print(\"\\n‚úÖ SUCCESS MODE: Trees improve over LR\")\n",
    "    print(\"   ‚Üí DIAGNOSIS: Non-linear interactions present\")\n",
    "    print(\"   ‚Üí ACTION: Use tree model BUT explain trade-offs\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚úÖ SUCCESS MODE: Similar performance\")\n",
    "    print(\"   ‚Üí DIAGNOSIS: Both models capture patterns well\")\n",
    "    print(\"   ‚Üí ACTION: Choose based on interpretability needs\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa09108",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ FINAL MODEL SELECTION & DOCUMENTATION\n",
    "\n",
    "### What You Document at End of TAB 5:\n",
    "\n",
    "1. ‚úÖ Which model is **baseline** (Dumb + Logistic Regression)\n",
    "2. ‚úÖ Which model is **chosen**\n",
    "3. ‚úÖ **Why** it was chosen\n",
    "4. ‚úÖ What **metric** mattered most\n",
    "5. ‚úÖ What **threshold strategy** is used\n",
    "\n",
    "**This is more important than code.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3947a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Selection\n",
    "print(\"=\" * 80)\n",
    "print(\"üèÜ FINAL MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Selection criteria: Prioritize Recall (catching churners) while maintaining reasonable precision\n",
    "# Business context: False negatives (missing churners) are more expensive than false positives\n",
    "\n",
    "# Find model with best recall among those with AUC > 0.70\n",
    "viable_models = results_df[results_df['ROC-AUC'] >= 0.70]\n",
    "\n",
    "if len(viable_models) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: No models achieved ROC-AUC >= 0.70\")\n",
    "    print(\"   Selecting best available model...\")\n",
    "    viable_models = results_df\n",
    "\n",
    "# Among viable models, select based on recall (primary) and precision (secondary)\n",
    "viable_models['Score'] = viable_models['Recall'] * 0.6 + viable_models['Precision'] * 0.4\n",
    "\n",
    "selected_model_name = viable_models['Score'].idxmax()\n",
    "selected_metrics = results_df.loc[selected_model_name]\n",
    "\n",
    "print(f\"\\nüéØ SELECTED MODEL: {selected_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ROC-AUC:   {selected_metrics['ROC-AUC']:.4f}\")\n",
    "print(f\"Recall:    {selected_metrics['Recall']:.4f}\")\n",
    "print(f\"Precision: {selected_metrics['Precision']:.4f}\")\n",
    "print(f\"F1-Score:  {selected_metrics['F1-Score']:.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìù SELECTION RATIONALE:\")\n",
    "if selected_model_name == 'Logistic Regression':\n",
    "    print(\"   ‚úÖ Interpretable and probability-calibrated\")\n",
    "    print(\"   ‚úÖ Easy to explain to business stakeholders\")\n",
    "    print(\"   ‚úÖ Stable and consistent predictions\")\n",
    "    print(\"   ‚úÖ Sufficient performance for business needs\")\n",
    "else:\n",
    "    improvement = selected_metrics['ROC-AUC'] - results_df.loc['Logistic Regression', 'ROC-AUC']\n",
    "    print(f\"   ‚úÖ Improved ROC-AUC by {improvement:.4f} over Logistic Regression\")\n",
    "    print(f\"   ‚úÖ Better recall: {selected_metrics['Recall']:.4f} vs {results_df.loc['Logistic Regression', 'Recall']:.4f}\")\n",
    "    print(\"   ‚ö†Ô∏è Trade-off: Less interpretable than Logistic Regression\")\n",
    "    print(\"   ‚úÖ Performance gain justifies complexity\")\n",
    "\n",
    "print(\"\\nüéØ THRESHOLD STRATEGY:\")\n",
    "print(f\"   Business-aligned threshold: {business_threshold:.3f}\")\n",
    "print(f\"   Target: Contact top 20% highest-risk customers\")\n",
    "print(f\"   Expected recall: ~{business_recall*100:.1f}%\")\n",
    "print(f\"   Expected precision: ~{business_precision*100:.1f}%\")\n",
    "\n",
    "print(\"\\nüìä BUSINESS IMPACT:\")\n",
    "total_churners = cm_business[1,0] + cm_business[1,1]\n",
    "churners_saved = cm_business[1,1]\n",
    "customers_contacted = cm_business[0,1] + cm_business[1,1]\n",
    "\n",
    "print(f\"   Churners saved: {churners_saved:,} / {total_churners:,} ({business_recall*100:.1f}%)\")\n",
    "print(f\"   Customers contacted: {customers_contacted:,} / {len(y_test):,} ({customers_contacted/len(y_test)*100:.1f}%)\")\n",
    "print(f\"   Efficiency: {business_precision*100:.1f}% of contacted customers are actual churners\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6d47a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ TAB 5 COMPLETE\n",
    "\n",
    "### What We've Accomplished:\n",
    "\n",
    "1. ‚úÖ **Dumb Baseline** - Established floor (83.16% accuracy, 0% recall)\n",
    "2. ‚úÖ **Logistic Regression** - First real model with interpretability\n",
    "3. ‚úÖ **Threshold Tuning** - Business-aligned decision boundary\n",
    "4. ‚úÖ **Tree Ensembles** - Tested XGBoost, LightGBM, CatBoost, RF, GB\n",
    "5. ‚úÖ **Model Comparison** - Evaluated on business-relevant metrics\n",
    "6. ‚úÖ **Final Selection** - Justified model choice with clear rationale\n",
    "\n",
    "---\n",
    "\n",
    "### Key Deliverables:\n",
    "\n",
    "üìå **Baseline Model:** Logistic Regression (interpretable, stable)  \n",
    "üìå **Selected Model:** [See above]  \n",
    "üìå **Threshold:** Business-aligned (top 20% risk)  \n",
    "üìå **Metrics:** ROC-AUC, Recall, Precision (NOT accuracy)  \n",
    "üìå **Zero Leakage:** All features from TAB 4  \n",
    "üìå **Interview-Safe Story:** Complete and justified\n",
    "\n",
    "---\n",
    "\n",
    "### Interview-Ready Statement:\n",
    "\n",
    "> \"We started with a dumb baseline that achieved 83% accuracy but caught zero churners.  \n",
    "> Our Logistic Regression baseline improved recall to ~XX% with ROC-AUC of X.XXX.  \n",
    "> After testing tree ensembles, we selected **[MODEL_NAME]** which achieved XX% recall.  \n",
    "> We tuned the threshold to X.XXX to contact the top 20% highest-risk customers,  \n",
    "> catching ~XX% of churners while maintaining XX% precision.\"\n",
    "\n",
    "**Note:** Fill in actual values after running the notebook.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps (TAB 6):\n",
    "\n",
    "**üß† Explainability + Business Action Plan**\n",
    "\n",
    "1. Feature importance (coefficients for LR, SHAP for trees)\n",
    "2. Business interpretation of key drivers\n",
    "3. Actionable retention strategies\n",
    "4. Cost-benefit analysis\n",
    "5. Deployment recommendations\n",
    "\n",
    "**This is where churn projects become decision systems, not models.**\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Your Move\n",
    "\n",
    "Ready for **TAB 6**?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a36d2",
   "metadata": {},
   "source": [
    "## üíæ Save Results for TAB 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f284ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model comparison results\n",
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save results dataframe\n",
    "results_df.to_csv('../data/processed/model_comparison_results.csv')\n",
    "print(\"‚úÖ Model comparison results saved to: ../data/processed/model_comparison_results.csv\")\n",
    "\n",
    "# Save selected model info\n",
    "with open('../data/processed/selected_model_info.txt', 'w') as f:\n",
    "    f.write(f\"Selected Model: {selected_model_name}\\n\")\n",
    "    f.write(f\"ROC-AUC: {selected_metrics['ROC-AUC']:.4f}\\n\")\n",
    "    f.write(f\"Recall: {selected_metrics['Recall']:.4f}\\n\")\n",
    "    f.write(f\"Precision: {selected_metrics['Precision']:.4f}\\n\")\n",
    "    f.write(f\"F1-Score: {selected_metrics['F1-Score']:.4f}\\n\")\n",
    "    f.write(f\"Business Threshold: {business_threshold:.3f}\\n\")\n",
    "\n",
    "print(\"‚úÖ Selected model info saved to: ../data/processed/selected_model_info.txt\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ TAB 5 COMPLETE - Ready for TAB 6 (Explainability)\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
